{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4718f587-e1e7-4fdf-84ef-ff3e4ff7d3df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/fb/12/2f5c8d4764b00033cf1c935b702d3bb878d10be9f0b87f0253495832d85f/sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fda3c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch loss: 19.879329681396484\n",
      "Epoch 1, Batch loss: 15.554410934448242\n",
      "Epoch 1, Batch loss: 13.111258506774902\n",
      "Epoch 1, Batch loss: 9.686079978942871\n",
      "Epoch 1, Batch loss: 10.359190940856934\n",
      "Epoch 1, Batch loss: 9.0573148727417\n",
      "Epoch 1, Batch loss: 5.73008918762207\n",
      "Epoch 1, Batch loss: 5.000347137451172\n",
      "Epoch 1, Batch loss: 4.214096546173096\n",
      "Epoch 1, Batch loss: 3.1553401947021484\n",
      "Epoch 1, Batch loss: 2.944056272506714\n",
      "Epoch 1, Batch loss: 2.4298644065856934\n",
      "Epoch 1, Batch loss: 2.574411630630493\n",
      "Epoch 1, Batch loss: 1.813261866569519\n",
      "Epoch 1, Batch loss: 1.8675488233566284\n",
      "Epoch 1, Batch loss: 1.7692523002624512\n",
      "Epoch 1, Batch loss: 1.605228304862976\n",
      "Epoch 1, Batch loss: 1.7078768014907837\n",
      "Epoch 1, Batch loss: 1.843479871749878\n",
      "Epoch 1, Batch loss: 1.6410244703292847\n",
      "Epoch 1, Batch loss: 1.4822709560394287\n",
      "Epoch 1, Batch loss: 1.6922506093978882\n",
      "Epoch 1, Batch loss: 1.5480772256851196\n",
      "Epoch 1, Batch loss: 1.287927508354187\n",
      "Epoch 1, Batch loss: 1.2570878267288208\n",
      "Epoch 1, Batch loss: 1.4817112684249878\n",
      "Epoch 1, Batch loss: 1.293808937072754\n",
      "Epoch 1, Batch loss: 1.357174277305603\n",
      "Epoch 1, Batch loss: 1.2355331182479858\n",
      "Epoch 1, Batch loss: 1.2712229490280151\n",
      "Epoch 1, Batch loss: 1.344967246055603\n",
      "Epoch 1, Batch loss: 1.275126576423645\n",
      "Epoch 1, Batch loss: 1.0822666883468628\n",
      "Epoch 1, Batch loss: 1.1551506519317627\n",
      "Epoch 1, Batch loss: 1.0796927213668823\n",
      "Epoch 1, Batch loss: 1.1255189180374146\n",
      "Epoch 1, Batch loss: 1.1126034259796143\n",
      "Epoch 1, Batch loss: 1.0662860870361328\n",
      "Epoch 1, Batch loss: 1.0200376510620117\n",
      "Epoch 1, Batch loss: 1.0460277795791626\n",
      "Epoch 1, Batch loss: 1.1991207599639893\n",
      "Epoch 1, Batch loss: 0.9182462692260742\n",
      "Epoch 1, Batch loss: 0.903114914894104\n",
      "Epoch 1, Batch loss: 0.9371448755264282\n",
      "Epoch 1, Batch loss: 1.040837049484253\n",
      "Epoch 1, Batch loss: 0.8858826160430908\n",
      "Epoch 1, Batch loss: 0.9388757944107056\n",
      "Epoch 1, Batch loss: 0.9770800471305847\n",
      "Epoch 1, Batch loss: 0.8238595128059387\n",
      "Epoch 1, Batch loss: 0.7116177678108215\n",
      "Epoch 1, Batch loss: 0.8440007567405701\n",
      "Epoch 1, Batch loss: 0.8721016645431519\n",
      "Epoch 1, Batch loss: 0.8105135560035706\n",
      "Epoch 1, Batch loss: 0.6665640473365784\n",
      "Epoch 1, Batch loss: 0.7380846738815308\n",
      "Epoch 1, Batch loss: 0.7732459902763367\n",
      "Epoch 1, Batch loss: 0.6783975958824158\n",
      "Epoch 1, Batch loss: 0.7592007517814636\n",
      "Epoch 1, Batch loss: 0.7682816386222839\n",
      "Epoch 1, Batch loss: 0.7209869027137756\n",
      "Epoch 1, Batch loss: 0.555812656879425\n",
      "Epoch 1, Batch loss: 0.6982235908508301\n",
      "Epoch 1, Batch loss: 0.7133563756942749\n",
      "Epoch 1, Batch loss: 0.5678879022598267\n",
      "Epoch 1, Batch loss: 0.6948363184928894\n",
      "Epoch 1, Batch loss: 0.5097343921661377\n",
      "Epoch 1, Batch loss: 0.4647514224052429\n",
      "Epoch 1, Batch loss: 0.43043574690818787\n",
      "Epoch 1, Batch loss: 0.8573471903800964\n",
      "Epoch 1, Batch loss: 0.655455470085144\n",
      "Epoch 1, Batch loss: 0.5847535133361816\n",
      "Epoch 1, Batch loss: 0.4295758008956909\n",
      "Epoch 1, Batch loss: 0.5975854396820068\n",
      "Epoch 1, Batch loss: 0.4834340810775757\n",
      "Epoch 1, Batch loss: 0.6307539939880371\n",
      "Epoch 1, Batch loss: 0.4947572648525238\n",
      "Epoch 1, Batch loss: 0.5023915767669678\n",
      "Epoch 1, Batch loss: 0.403837114572525\n",
      "Epoch 1, Batch loss: 0.4403328001499176\n",
      "Epoch 1, Batch loss: 0.4636991024017334\n",
      "Epoch 1, Batch loss: 0.4438292980194092\n",
      "Epoch 1, Batch loss: 0.4994676113128662\n",
      "Epoch 1, Batch loss: 0.2986202538013458\n",
      "Epoch 1, Batch loss: 0.4907311201095581\n",
      "Epoch 1, Batch loss: 0.6264155507087708\n",
      "Epoch 1, Batch loss: 0.4828225374221802\n",
      "Epoch 1, Batch loss: 0.6681287884712219\n",
      "Epoch 1, Batch loss: 0.4753021001815796\n",
      "Epoch 1, Batch loss: 0.4632197916507721\n",
      "Epoch 1, Batch loss: 0.6679164171218872\n",
      "Epoch 1, Batch loss: 0.49444612860679626\n",
      "Epoch 1, Batch loss: 0.5208216309547424\n",
      "Epoch 1, Batch loss: 0.5009461641311646\n",
      "Epoch 1, Batch loss: 0.474443644285202\n",
      "Epoch 1, Batch loss: 0.5060967206954956\n",
      "Epoch 1, Batch loss: 0.4908509850502014\n",
      "Epoch 1, Batch loss: 0.41172873973846436\n",
      "Epoch 1, Batch loss: 0.5222766995429993\n",
      "Epoch 1, Batch loss: 0.40438947081565857\n",
      "Epoch 1, Batch loss: 0.3475928008556366\n",
      "Epoch 1, Batch loss: 0.38094279170036316\n",
      "Epoch 1, Batch loss: 0.4124131202697754\n",
      "Epoch 1, Batch loss: 0.5686278939247131\n",
      "Epoch 1, Batch loss: 0.5357427000999451\n",
      "Epoch 1, Batch loss: 0.4083118438720703\n",
      "Epoch 1, Batch loss: 0.4119117856025696\n",
      "Epoch 1, Batch loss: 0.4288046061992645\n",
      "Epoch 1, Batch loss: 0.35210588574409485\n",
      "Epoch 1, Batch loss: 0.3393367826938629\n",
      "Epoch 1, Batch loss: 0.7029258012771606\n",
      "Epoch 1, Batch loss: 0.32075297832489014\n",
      "Epoch 1, Batch loss: 0.5532610416412354\n",
      "Epoch 1, Batch loss: 0.39659470319747925\n",
      "Epoch 1, Batch loss: 0.5349358916282654\n",
      "Epoch 1, Batch loss: 0.36740854382514954\n",
      "Epoch 1, Batch loss: 0.5606718063354492\n",
      "Epoch 1, Batch loss: 0.3600082993507385\n",
      "Epoch 1, Batch loss: 0.434787392616272\n",
      "Epoch 1, Batch loss: 0.37348973751068115\n",
      "Epoch 1, Batch loss: 0.5175370573997498\n",
      "Epoch 1, Batch loss: 0.3062589168548584\n",
      "Epoch 1, Batch loss: 0.45755332708358765\n",
      "Epoch 1, Batch loss: 0.3629576861858368\n",
      "Epoch 1, Batch loss: 0.42599350214004517\n",
      "Epoch 1, Batch loss: 0.6231287717819214\n",
      "Epoch 2, Batch loss: 0.3082596957683563\n",
      "Epoch 2, Batch loss: 0.27201956510543823\n",
      "Epoch 2, Batch loss: 0.33581626415252686\n",
      "Epoch 2, Batch loss: 0.4854680001735687\n",
      "Epoch 2, Batch loss: 0.43736177682876587\n",
      "Epoch 2, Batch loss: 0.39521411061286926\n",
      "Epoch 2, Batch loss: 0.509376049041748\n",
      "Epoch 2, Batch loss: 0.38887688517570496\n",
      "Epoch 2, Batch loss: 0.45001131296157837\n",
      "Epoch 2, Batch loss: 0.4785226285457611\n",
      "Epoch 2, Batch loss: 0.45224517583847046\n",
      "Epoch 2, Batch loss: 0.49029332399368286\n",
      "Epoch 2, Batch loss: 0.34532079100608826\n",
      "Epoch 2, Batch loss: 0.3789371848106384\n",
      "Epoch 2, Batch loss: 0.4949025809764862\n",
      "Epoch 2, Batch loss: 0.43011584877967834\n",
      "Epoch 2, Batch loss: 0.3648594617843628\n",
      "Epoch 2, Batch loss: 0.6174489259719849\n",
      "Epoch 2, Batch loss: 0.3128591477870941\n",
      "Epoch 2, Batch loss: 0.3195832669734955\n",
      "Epoch 2, Batch loss: 0.4270690083503723\n",
      "Epoch 2, Batch loss: 0.3943248987197876\n",
      "Epoch 2, Batch loss: 0.4026321470737457\n",
      "Epoch 2, Batch loss: 0.47089022397994995\n",
      "Epoch 2, Batch loss: 0.35030046105384827\n",
      "Epoch 2, Batch loss: 0.3008699417114258\n",
      "Epoch 2, Batch loss: 0.36187630891799927\n",
      "Epoch 2, Batch loss: 0.3834061026573181\n",
      "Epoch 2, Batch loss: 0.2753274142742157\n",
      "Epoch 2, Batch loss: 0.3614867627620697\n",
      "Epoch 2, Batch loss: 0.511579692363739\n",
      "Epoch 2, Batch loss: 0.42296621203422546\n",
      "Epoch 2, Batch loss: 0.42549121379852295\n",
      "Epoch 2, Batch loss: 0.6244616508483887\n",
      "Epoch 2, Batch loss: 0.3870619535446167\n",
      "Epoch 2, Batch loss: 0.4534927010536194\n",
      "Epoch 2, Batch loss: 0.4100564420223236\n",
      "Epoch 2, Batch loss: 0.4009269177913666\n",
      "Epoch 2, Batch loss: 0.3819112479686737\n",
      "Epoch 2, Batch loss: 0.6371668577194214\n",
      "Epoch 2, Batch loss: 0.30177024006843567\n",
      "Epoch 2, Batch loss: 0.39860019087791443\n",
      "Epoch 2, Batch loss: 0.32329609990119934\n",
      "Epoch 2, Batch loss: 0.4235008656978607\n",
      "Epoch 2, Batch loss: 0.24989597499370575\n",
      "Epoch 2, Batch loss: 0.3823155462741852\n",
      "Epoch 2, Batch loss: 0.3854774236679077\n",
      "Epoch 2, Batch loss: 0.4989840090274811\n",
      "Epoch 2, Batch loss: 0.4015611410140991\n",
      "Epoch 2, Batch loss: 0.37994322180747986\n",
      "Epoch 2, Batch loss: 0.3644449710845947\n",
      "Epoch 2, Batch loss: 0.28834477066993713\n",
      "Epoch 2, Batch loss: 0.27514544129371643\n",
      "Epoch 2, Batch loss: 0.34708601236343384\n",
      "Epoch 2, Batch loss: 0.4848490059375763\n",
      "Epoch 2, Batch loss: 0.3225783407688141\n",
      "Epoch 2, Batch loss: 0.30927368998527527\n",
      "Epoch 2, Batch loss: 0.33263301849365234\n",
      "Epoch 2, Batch loss: 0.4377638101577759\n",
      "Epoch 2, Batch loss: 0.41198983788490295\n",
      "Epoch 2, Batch loss: 0.5101786255836487\n",
      "Epoch 2, Batch loss: 0.573158323764801\n",
      "Epoch 2, Batch loss: 0.34638649225234985\n",
      "Epoch 2, Batch loss: 0.5097030401229858\n",
      "Epoch 2, Batch loss: 0.3918512165546417\n",
      "Epoch 2, Batch loss: 0.3363170623779297\n",
      "Epoch 2, Batch loss: 0.34383127093315125\n",
      "Epoch 2, Batch loss: 0.3384180963039398\n",
      "Epoch 2, Batch loss: 0.3927893042564392\n",
      "Epoch 2, Batch loss: 0.4471242427825928\n",
      "Epoch 2, Batch loss: 0.3414181172847748\n",
      "Epoch 2, Batch loss: 0.3673665225505829\n",
      "Epoch 2, Batch loss: 0.5145216584205627\n",
      "Epoch 2, Batch loss: 0.37698715925216675\n",
      "Epoch 2, Batch loss: 0.37845370173454285\n",
      "Epoch 2, Batch loss: 0.40825334191322327\n",
      "Epoch 2, Batch loss: 0.3849959969520569\n",
      "Epoch 2, Batch loss: 0.43249815702438354\n",
      "Epoch 2, Batch loss: 0.3142769932746887\n",
      "Epoch 2, Batch loss: 0.24161361157894135\n",
      "Epoch 2, Batch loss: 0.3093639016151428\n",
      "Epoch 2, Batch loss: 0.38264816999435425\n",
      "Epoch 2, Batch loss: 0.38913074135780334\n",
      "Epoch 2, Batch loss: 0.27793821692466736\n",
      "Epoch 2, Batch loss: 0.40914633870124817\n",
      "Epoch 2, Batch loss: 0.5736014246940613\n",
      "Epoch 2, Batch loss: 0.2950742244720459\n",
      "Epoch 2, Batch loss: 0.5552632212638855\n",
      "Epoch 2, Batch loss: 0.28242066502571106\n",
      "Epoch 2, Batch loss: 0.4590471684932709\n",
      "Epoch 2, Batch loss: 0.3309318721294403\n",
      "Epoch 2, Batch loss: 0.32288116216659546\n",
      "Epoch 2, Batch loss: 0.4149710237979889\n",
      "Epoch 2, Batch loss: 0.33019331097602844\n",
      "Epoch 2, Batch loss: 0.37780046463012695\n",
      "Epoch 2, Batch loss: 0.5555858612060547\n",
      "Epoch 2, Batch loss: 0.5933305621147156\n",
      "Epoch 2, Batch loss: 0.5207289457321167\n",
      "Epoch 2, Batch loss: 0.5777199268341064\n",
      "Epoch 2, Batch loss: 0.4391094744205475\n",
      "Epoch 2, Batch loss: 0.2933040261268616\n",
      "Epoch 2, Batch loss: 0.3672131896018982\n",
      "Epoch 2, Batch loss: 0.4365740120410919\n",
      "Epoch 2, Batch loss: 0.33741798996925354\n",
      "Epoch 2, Batch loss: 0.24195559322834015\n",
      "Epoch 2, Batch loss: 0.2870544195175171\n",
      "Epoch 2, Batch loss: 0.4497174620628357\n",
      "Epoch 2, Batch loss: 0.43865031003952026\n",
      "Epoch 2, Batch loss: 0.3016381859779358\n",
      "Epoch 2, Batch loss: 0.37040120363235474\n",
      "Epoch 2, Batch loss: 0.3316362798213959\n",
      "Epoch 2, Batch loss: 0.5452425479888916\n",
      "Epoch 2, Batch loss: 0.3802063465118408\n",
      "Epoch 2, Batch loss: 0.312934011220932\n",
      "Epoch 2, Batch loss: 0.4169151186943054\n",
      "Epoch 2, Batch loss: 0.3167174756526947\n",
      "Epoch 2, Batch loss: 0.254491925239563\n",
      "Epoch 2, Batch loss: 0.2928648591041565\n",
      "Epoch 2, Batch loss: 0.3180738687515259\n",
      "Epoch 2, Batch loss: 0.5311344861984253\n",
      "Epoch 2, Batch loss: 0.4490421712398529\n",
      "Epoch 2, Batch loss: 0.2463221251964569\n",
      "Epoch 2, Batch loss: 0.3155222237110138\n",
      "Epoch 2, Batch loss: 0.4535961449146271\n",
      "Epoch 2, Batch loss: 0.25652724504470825\n",
      "Epoch 3, Batch loss: 0.3471827208995819\n",
      "Epoch 3, Batch loss: 0.5166987180709839\n",
      "Epoch 3, Batch loss: 0.28303608298301697\n",
      "Epoch 3, Batch loss: 0.29473114013671875\n",
      "Epoch 3, Batch loss: 0.30738040804862976\n",
      "Epoch 3, Batch loss: 0.3404020071029663\n",
      "Epoch 3, Batch loss: 0.43485358357429504\n",
      "Epoch 3, Batch loss: 0.39893603324890137\n",
      "Epoch 3, Batch loss: 0.3472879230976105\n",
      "Epoch 3, Batch loss: 0.3179638385772705\n",
      "Epoch 3, Batch loss: 0.2595096528530121\n",
      "Epoch 3, Batch loss: 0.4779289662837982\n",
      "Epoch 3, Batch loss: 0.41151854395866394\n",
      "Epoch 3, Batch loss: 0.3408777117729187\n",
      "Epoch 3, Batch loss: 0.3214898705482483\n",
      "Epoch 3, Batch loss: 0.2981184124946594\n",
      "Epoch 3, Batch loss: 0.38127779960632324\n",
      "Epoch 3, Batch loss: 0.31981614232063293\n",
      "Epoch 3, Batch loss: 0.2954709231853485\n",
      "Epoch 3, Batch loss: 0.39715051651000977\n",
      "Epoch 3, Batch loss: 0.35761675238609314\n",
      "Epoch 3, Batch loss: 0.48478761315345764\n",
      "Epoch 3, Batch loss: 0.41026145219802856\n",
      "Epoch 3, Batch loss: 0.41480958461761475\n",
      "Epoch 3, Batch loss: 0.30304089188575745\n",
      "Epoch 3, Batch loss: 0.2822631597518921\n",
      "Epoch 3, Batch loss: 0.3248801529407501\n",
      "Epoch 3, Batch loss: 0.2969748079776764\n",
      "Epoch 3, Batch loss: 0.4167986810207367\n",
      "Epoch 3, Batch loss: 0.29663562774658203\n",
      "Epoch 3, Batch loss: 0.3650934398174286\n",
      "Epoch 3, Batch loss: 0.32894089818000793\n",
      "Epoch 3, Batch loss: 0.35726398229599\n",
      "Epoch 3, Batch loss: 0.40854156017303467\n",
      "Epoch 3, Batch loss: 0.6758639216423035\n",
      "Epoch 3, Batch loss: 0.3533441424369812\n",
      "Epoch 3, Batch loss: 0.24876856803894043\n",
      "Epoch 3, Batch loss: 0.3808809220790863\n",
      "Epoch 3, Batch loss: 0.30891966819763184\n",
      "Epoch 3, Batch loss: 0.45176962018013\n",
      "Epoch 3, Batch loss: 0.4769872725009918\n",
      "Epoch 3, Batch loss: 0.4570431113243103\n",
      "Epoch 3, Batch loss: 0.2494911253452301\n",
      "Epoch 3, Batch loss: 0.3291553854942322\n",
      "Epoch 3, Batch loss: 0.47495728731155396\n",
      "Epoch 3, Batch loss: 0.3258225917816162\n",
      "Epoch 3, Batch loss: 0.6057875156402588\n",
      "Epoch 3, Batch loss: 0.2666751742362976\n",
      "Epoch 3, Batch loss: 0.24119842052459717\n",
      "Epoch 3, Batch loss: 0.25456473231315613\n",
      "Epoch 3, Batch loss: 0.34612518548965454\n",
      "Epoch 3, Batch loss: 0.5299559235572815\n",
      "Epoch 3, Batch loss: 0.3110516369342804\n",
      "Epoch 3, Batch loss: 0.3741413652896881\n",
      "Epoch 3, Batch loss: 0.357871413230896\n",
      "Epoch 3, Batch loss: 0.2501809000968933\n",
      "Epoch 3, Batch loss: 0.2672363221645355\n",
      "Epoch 3, Batch loss: 0.3210715651512146\n",
      "Epoch 3, Batch loss: 0.32578662037849426\n",
      "Epoch 3, Batch loss: 0.33493390679359436\n",
      "Epoch 3, Batch loss: 0.3245103657245636\n",
      "Epoch 3, Batch loss: 0.4636442959308624\n",
      "Epoch 3, Batch loss: 0.44421279430389404\n",
      "Epoch 3, Batch loss: 0.5794335603713989\n",
      "Epoch 3, Batch loss: 0.398850679397583\n",
      "Epoch 3, Batch loss: 0.31892314553260803\n",
      "Epoch 3, Batch loss: 0.3128461241722107\n",
      "Epoch 3, Batch loss: 0.32231518626213074\n",
      "Epoch 3, Batch loss: 0.3523911237716675\n",
      "Epoch 3, Batch loss: 0.2383994311094284\n",
      "Epoch 3, Batch loss: 0.39939117431640625\n",
      "Epoch 3, Batch loss: 0.258548378944397\n",
      "Epoch 3, Batch loss: 0.2927090525627136\n",
      "Epoch 3, Batch loss: 0.3109980523586273\n",
      "Epoch 3, Batch loss: 0.2715948820114136\n",
      "Epoch 3, Batch loss: 0.3133277893066406\n",
      "Epoch 3, Batch loss: 0.3123471736907959\n",
      "Epoch 3, Batch loss: 0.30150386691093445\n",
      "Epoch 3, Batch loss: 0.2844125032424927\n",
      "Epoch 3, Batch loss: 0.3540782034397125\n",
      "Epoch 3, Batch loss: 0.2946087121963501\n",
      "Epoch 3, Batch loss: 0.35498130321502686\n",
      "Epoch 3, Batch loss: 0.3041574954986572\n",
      "Epoch 3, Batch loss: 0.4388747811317444\n",
      "Epoch 3, Batch loss: 0.4556429386138916\n",
      "Epoch 3, Batch loss: 0.3180895745754242\n",
      "Epoch 3, Batch loss: 0.2966166138648987\n",
      "Epoch 3, Batch loss: 0.28056585788726807\n",
      "Epoch 3, Batch loss: 0.27801427245140076\n",
      "Epoch 3, Batch loss: 0.3600628972053528\n",
      "Epoch 3, Batch loss: 0.3253668248653412\n",
      "Epoch 3, Batch loss: 0.3901430666446686\n",
      "Epoch 3, Batch loss: 0.31057876348495483\n",
      "Epoch 3, Batch loss: 0.5820469260215759\n",
      "Epoch 3, Batch loss: 0.29248735308647156\n",
      "Epoch 3, Batch loss: 0.32197386026382446\n",
      "Epoch 3, Batch loss: 0.4391528367996216\n",
      "Epoch 3, Batch loss: 0.2795533239841461\n",
      "Epoch 3, Batch loss: 0.39074328541755676\n",
      "Epoch 3, Batch loss: 0.34321826696395874\n",
      "Epoch 3, Batch loss: 0.3921574652194977\n",
      "Epoch 3, Batch loss: 0.21536237001419067\n",
      "Epoch 3, Batch loss: 0.30422449111938477\n",
      "Epoch 3, Batch loss: 0.3563991189002991\n",
      "Epoch 3, Batch loss: 0.3277837336063385\n",
      "Epoch 3, Batch loss: 0.25063905119895935\n",
      "Epoch 3, Batch loss: 0.2334061712026596\n",
      "Epoch 3, Batch loss: 0.338357150554657\n",
      "Epoch 3, Batch loss: 0.37023693323135376\n",
      "Epoch 3, Batch loss: 0.39423733949661255\n",
      "Epoch 3, Batch loss: 0.2643316984176636\n",
      "Epoch 3, Batch loss: 0.27233996987342834\n",
      "Epoch 3, Batch loss: 0.354455828666687\n",
      "Epoch 3, Batch loss: 0.20526652038097382\n",
      "Epoch 3, Batch loss: 0.27832525968551636\n",
      "Epoch 3, Batch loss: 0.6219280958175659\n",
      "Epoch 3, Batch loss: 0.42026540637016296\n",
      "Epoch 3, Batch loss: 0.3921284079551697\n",
      "Epoch 3, Batch loss: 0.28572601079940796\n",
      "Epoch 3, Batch loss: 0.4199543297290802\n",
      "Epoch 3, Batch loss: 0.2317492663860321\n",
      "Epoch 3, Batch loss: 0.3815740942955017\n",
      "Epoch 3, Batch loss: 0.21205152571201324\n",
      "Epoch 3, Batch loss: 0.3375987410545349\n",
      "Epoch 3, Batch loss: 0.24036911129951477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_t5/tokenizer_config.json',\n",
       " 'fine_tuned_t5/special_tokens_map.json',\n",
       " 'fine_tuned_t5/spiece.model',\n",
       " 'fine_tuned_t5/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import pandas as pd  # Add this line to import pandas\n",
    "# Define your CSV file path\n",
    "csv_file = \"/student/mjr175/commentGeneration/Train_4_Lang/Java1000Train.csv\"\n",
    "\n",
    "\n",
    "# Create a custom dataset class\n",
    "class SentenceConversionDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gptComment, groundTruth = self.data.loc[idx, \"gptComment\"], self.data.loc[idx, \"groundTruth\"]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            f\"convert gptComment to groundTruth: {gptComment}\",\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer.encode(\n",
    "            groundTruth,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels.squeeze()\n",
    "        }\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # or \"t5-large\", \"t5-3b\", \"t5-11b\", \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define your batch size and maximum sequence length\n",
    "batch_size = 8\n",
    "max_length = 256\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = SentenceConversionDataset(csv_file, tokenizer, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define optimizer and learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Batch loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_t5\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_t5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f4a646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: This function sends an HTTP DELETE request to the specified URI, retrieves the response, and returns it, ensuring proper resource cleanup after execution.\n",
      "Generated humanLike comment: In this case, the HTTP DELETE request is sent to the specified URI.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model = T5ForConditionalGeneration.from_pretrained(\"fine_tuned_t5\")\n",
    "fine_tuned_tokenizer = T5Tokenizer.from_pretrained(\"fine_tuned_t5\")\n",
    "\n",
    "# Test the model\n",
    "test_sentence = \"This function sends an HTTP DELETE request to the specified URI, retrieves the response, and returns it, ensuring proper resource cleanup after execution.\"\n",
    "inputs = fine_tuned_tokenizer.encode_plus(\n",
    "    f\"convert gptComment to humanLikeComment: {test_sentence}\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# Ensure tensors are on the same device as the model (cuda or cpu)\n",
    "fine_tuned_model.to(device)\n",
    "\n",
    "output_ids = fine_tuned_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,  # Adjust the max_length as needed\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "decoded_output = fine_tuned_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Input sentence:\", test_sentence)\n",
    "print(\"Generated humanLike comment:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d591778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
